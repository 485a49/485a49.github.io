{"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/ttp:/example.com/2024/12/01/hello-world/"},{"title":"深入解析 GPT 的工作原理","text":"前言GPT（Generative Pretrained Transformer）是一种基于 Transformer 架构 的自然语言处理模型。通过对用户输入的文本进行语义分析，GPT 能够生成连贯、符合上下文的回答。为了更好地理解 GPT 的工作原理，我们将其分为四个主要步骤：输入处理、Transformer 内部计算、自注意力机制、输出生成。本文将逐步解析这些步骤，尤其是 Transformer 的详细结构和机制，并通过具体示例说明每一步的作用。 输入处理：从自然语言到机器可理解的形式在用户向 GPT 提出问题时，模型需要首先将这些自然语言转换为机器能够处理的数值表示。这个过程涉及两个关键步骤：令牌化（Tokenization） 和 嵌入表示（Embeddings）。 令牌化是将输入文本分解为更小的单元——令牌（tokens）。这些令牌可以是单词、子词或字符片段。GPT 将输入文本逐词或逐子词分解，以便模型能够更细致地处理文本结构。 比如对于句子“为什么天空是蓝色的？”，令牌化后，可能被分解为 [为什么，天空，是，蓝色。的，?]。 为了提高处理能力，GPT 使用 子词令牌化方法，对于较长的词汇，模型可能将其拆解为多个子词。 将令牌化后的单词转化为数值表示是自然语言处理（NLP）中至关重要的一步。每个令牌通过嵌入层被映射为一个高维向量，向量的每个维度捕捉了该词的某种语义信息。这使得 GPT 能够处理自然语言的语义关系。这个步骤通常称为词嵌入（word embedding）或向量化（vectorization）。 这个过程有几种常见的实现方法： 独热编码（One-hot Encoding）：为每个词汇分配一个唯一的数字位置。每个令牌会转化为一个向量，其中只有对应单词位置的元素为 1，其它元素为 0。其优点在于简单直观，缺点在于高维稀疏，无法捕获词义。 词袋模型（Bag of Words，BoW）：通过统计文本中各个词汇的出现次数来表示文本。每个文档或句子被表示为一个词汇表中单词频率的向量。词袋模型忽略了单词的顺序，无法捕捉语法或上下文信息。当词汇表很大时，生成的向量会非常稀疏。 词嵌入（Word Embedding）：将每个单词映射到一个低维度的连续向量空间中。这些向量不仅具有数值表示，而且能够捕捉单词之间的语义和关系。流行的词嵌入方式包括： Word2Vec：通过上下文窗口来训练单词的向量表示，能够捕捉到单词间的相似性。例如，“king” - “man” + “woman” ≈ “queen”。 GloVe（Global Vectors for Word Representation）：基于统计信息和词频共现关系训练词向量。 FastText：不仅考虑单词，还考虑单词内部的子词，能够处理词形变化和拼写错误。 通过嵌入表示，模型能够理解词语的语义，并为后续的计算奠定基础。 Transformer 内部计算：核心架构的处理嵌入后的数值表示会被传入 GPT 的 Transformer 架构中，这一部分是模型理解输入的核心。Transformer 通过多层计算对输入句子进行深度分析，并生成相应的输出。 Transformer 是一种深度学习模型架构，首次由 Vaswani 等人于 2017 年在论文 Attention is All You Need 中提出。 Transformer 是由多层堆叠的 编码器-解码器（Encoder-Decoder）结构 演化而来，但 GPT 仅使用其中的 解码器 部分。Transformer 的每一层都包含两个主要组成部分： 自注意力机制（Self-Attention）：用来捕捉输入中词与词之间的关联。 前馈神经网络（Feed-Forward Network, FFN）：用来进行进一步的非线性变换，从而丰富语义表示。 在每一层 Transformer 中，嵌入表示被送入 自注意力机制，通过权重计算词语之间的关系。然后，经过前馈神经网络进一步处理。每一层的输出会作为下一层的输入，逐步强化对文本的语义理解。 这种多层处理使得 GPT 能够从表面词汇关联到更深层的语义关系，增强了其语言生成能力。 自注意力机制：模型理解上下文的基础自注意力机制是 Transformer 架构的关键组件，它使得 GPT 在生成每个词时，能够 “关注” 到与当前词相关的其他词。通过这种机制，模型可以捕捉到句子中远距离词语之间的依赖关系，并理解复杂的句子结构。 自注意力机制的工作原理可以这样表示。假设输入序列为：$[X_1, X_2, X_3,\\dots, X_n]$，其中 $X$ 表示每个词的嵌入表示。自注意力机制的目标是计算每个单词（例如 $X_1$）如何与其他单词（例如 $X_2$, $X_3$ 等）进行交互，并调整其表示。 每个输入单词的向量表示会被映射成三个不同的向量： 查询（Query）：表示当前词的查询向量。 键（Key）：表示其他词的键向量。 值（Value）：表示其他词的值向量。 这些向量是通过将输入词向量与学习到的权重矩阵相乘得到的。 模型计算每个查询（Query）和所有键（Key）之间的相似度，通常使用点积计算。这个得分表示了当前词对其他词的“注意力”程度。$$score(Q_i,K_j)=Q_i\\cdot K_j^T$$得分越高，表示 $Q_i$ 更加关注 $K_j$。 为了避免点积结果过大，通常会将得分除以一个常数（通常是键向量的维度的平方根）进行缩放：$$scaled_score(Q_i,K_j)=\\frac{Q_i\\cdot K_j^T}{\\sqrt{d_k}}⋅$$其中，$d_k$ 是键向量的维度。 对每个查询向量的得分进行 Softmax 操作，以确保所有的注意力得分和为 1，从而转化为概率分布：$$Attention_output_i=\\sum_jAttention_weights(Q_i,K_j)\\cdot V_j$$这一步骤的结果就是调整后的表示，它包含了序列中所有其他词的信息。 为了让模型能够在不同的子空间（不同角度）中学习信息，Transformer 引入了 多头注意力。它将查询、键和值分别拆分成多个头，每个头独立计算注意力，然后将所有头的结果拼接起来，经过线性变换得到最终的输出。 输出生成：从预测到生成完整回答经过多层 Transformer 和自注意力机制的处理，GPT 对输入的语义有了充分的理解，接下来进入 输出生成 阶段。GPT 的输出生成过程是逐词生成，并结合上下文进行推测。 GPT 使用自回归生成，即逐词生成下一个词，生成每个词时都会参考前面的上下文。每一步都会根据当前词语及其上下文信息，预测下一个最有可能的词。 在每次生成一个词时，GPT 会为所有可能的下一个词计算出一个 概率分布。每个词都有一定的概率被选中，模型会根据这些概率决定哪个词最适合作为下一个输出。 温度参数（Temperature Parameter）是控制文本生成模型（如 GPT、LSTM 等）输出随机性的一个超参数。 给定一个词的 logits（未经归一化的预测值），softmax 函数的输出是每个单词的概率：$$P(w_i)=\\frac{e^{z_i}}{\\sum_je^{z_j}}$$其中 $z_i$ 为对应单词的 logit。温度参数 $T$ 的引入修改了 softmax 函数，使得每个 logit 被除以一个温度值：$$P(w_i)=\\frac{e^{z_i/T}}{\\sum_je^{z_j/T}}$$当温度值低于 1 时，模型的概率分布变得更加“尖锐”。这意味着生成高概率单词的机会会增加，模型更倾向于选择最可能的单词。当温度值大于 1 时，模型生成低概率词的机会增大，生成的文本会更加随机、多样化，温度为 1 时，生成的概率分布是标准的，没有任何调节。","link":"/ttp:/example.com/2024/12/01/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90-GPT-%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/"},{"title":"详细设计","text":"详细设计cn.edu.nwafu.cie.teach.common.util.RedisDistributedLock 通过 SET 命令的原子性与 Lua 脚本的事务特性，保证了分布式环境中锁的正确性与安全性。 1234if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end Nacos 源码化运行1. 方便开发过程使用如果从 Spring Cloud Netflix 体系迁移到 Spring Cloud Alibaba 技术体系，明显的感受是整个体系得到简化。 Nacos 承担整个 Spring Cloud 的服务发现、配置管理部分的实现。 是整个开发过程中强依赖，启动微服务业务要去检查 Nacos Server 是否已经启动，解压安装的方式变的非常不便。 如果把 Nacos Server 作为整个微服务框架的一部分直接 Main 启动，是不是更加方便便利。 2. UI 个性定制化若以解压运行方式，修改 UI 几乎不可能。可以下载 Nacos 源码继续修改 然后重新打包运行。 非常的不方便 123456789git clone https://github.com/alibaba/nacos.gitcd nacos/mvn -Prelease-nacos -Dmaven.test.skip=true clean install -Uls -al distribution/target/// change the $version to your actual pathcd distribution/target/nacos-server-$version/nacos/bin 3. 保证 Server &amp; Client 保持一致 pig 作为微服务开源项目，更新迭代速度非常快。每个版本依赖的 Nacos Client 版本都可能发生变化，这就意味着对应的 Nacos Server 版本也要对应升级，这需要用户自行下载升级成本很高。 Nacos 具有良好小版本向下兼容性，但是大版本功能变化挺大，比如 1.2 、1.3 权限的变更。所以建议大家在实际开发过程中保持版本一致。 若以源码运行的方式，可以很好的解决此问题。","link":"/ttp:/example.com/2024/12/01/%E8%AF%A6%E7%BB%86%E8%AE%BE%E8%AE%A1/"}],"tags":[],"categories":[],"pages":[]}