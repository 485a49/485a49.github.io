{"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2024/12/01/hello-world/"},{"title":"设计模式","text":"设计模式（Design pattern）是一套被反复使用、多数人知晓的、经过分类编目的、代码设计经验的总结创建型模式。 设计原则设计模式中主要有六大设计原则，简称为 SOLID。 单一职责原则：一个类只负责完成一个职责或者功能。 开放封闭原则：对扩展开放，对修改关闭。 里氏替换原则：替换的前提是面向对象语言所支持的多态特性，同一个行为具有多个不同表现形式或形态的能力。在不了解派生类的情况下，仅通过接口或基类的方法，即可清楚的知道方法的行为，而不管哪种派生类的实现，都与接口或基类方法的期望行为一致。 接口隔离原则：要为各个类建立它们需要的专用接口，而不要试图去建立一个很庞大的接口供所有依赖它的类去调用。 依赖倒置原则：依赖倒置原则（Dependence Inversion Principle，DIP）是指在设计代码架构时，高层模块不应该依赖于底层模块，二者都应该依赖于抽象。抽象不应该依赖于细节，细节应该依赖于抽象。 迪米特法则：不该有直接依赖关系的类之间，不要有依赖；有依赖关系的类之间，尽量只依赖必要的接口。 单例模式单例模式（Singleton Pattern）保证只有一个实例存在，整个系统只能使用一个对象实例。有饿汉式（非线程安全）、懒汉式（线程安全）、双检锁（线程安全）。 懒汉式线程不安全，所以严格意义上它并不算单例模式。 1234567891011121314public class Singleton { private final static Singleton1 INSTANCE = new Singleton1(); /**static { INSTANCE = new Singleton2(); }*/ private Singleton() { } public static Singleton getInstance() { return INSTANCE; }} 饿汉式线程安全，必须加锁 synchronized 才能保证单例，但加锁会影响效率。 1234567891011121314public class Singleton4 { private static Singleton4 instance; private Singleton4() { } public synchronized static Singleton4 getInstance() { if (instance == null) { instance = new Singleton4(); } return instance; }} 同步效率过低。 双重检查123456789101112131415161718192021222324public class Singleton { // 防止指令重排，不是原子操作，赋值分为 3 步骤： 分配内存，调用构造方法，把引用指向分配的内存空间 private volatile static Singleton instance; private Singleton() { } public static Singleton getInstance() { // 若为空，无需进入同步代码块，提升效率 if (instance == null) { // 防止多线程情况下出现实例化多个实例的情况 synchronized (Singleton6.class) { // 同步后再次检查以确保实例未被创建 if (instance == null) { // 新建实例 instance = new Singleton6(); } } } return instance; }} 优点在于线程安全，延迟加载，效率较高。可以保证线程安全和性能。volatile 创建对象不是原子操作，防止指令重排序。 枚举123456public enum Singleton { INSTANCE; public void getInstance() { }} 由于枚举类的特殊性质，保证其线程安全（只有一个实例被创建），自带私有的构造方法并且序列化和反射都不会破坏单例的安全性。 工厂模式简单工厂模式简单工厂模式（Simple Factory Pattern）又称为静态工厂方法（Static Factory Method）模式，它属于类创建型模式。在简单工厂模式中，可以根据参数的不同返回不同类的实例。 12345678910111213141516171819202122232425262728293031323334353637383940// 产品接口public interface ICar { void drive();}// 若干具体的产品 public class Bmw implements ICar { @Override public void drive() { System.out.println(&quot;drive a Bmw&quot;); }}public class Benz implements ICar { @Override public void drive() { System.out.println(&quot;drive a Benz&quot;); }}public class Audi implements ICar { @Override public void drive() { System.out.println(&quot;drive a Audi&quot;); }}// 工厂类 public class CarFactory { private CarFactory() { } // 工厂不需要有状态，因此创造产品的方法是静态的 public static ICar create(String car) { if (&quot;Bmw&quot;.equalsIgnoreCase(car)) { return new Bmw(); // 后续可在工厂里变更产品类名、构造方法参数甚至实例化方式 } else if (&quot;Benz&quot;.equalsIgnoreCase(car)) { return new Benz(); } else if(&quot;Audi&quot;.equalsIgnoreCase(car)) { return new Audi(); } else { return null; } } } 抽象工厂模式抽象工厂模式（Abstract Factory Pattern）是围绕一个超级工厂创建其他工厂。该超级工厂又称为其他工厂的工厂。 原型模式结构型模式装饰器模式装饰器（Decorator）模式可以在不改变原有对象的情况下拓展其功能。 对于字节流来说， FilterInputStream 和 FilterOutputStream 是装饰器模式的核心，分别用于增强 InputStream 和 OutputStream 子类对象的功能。 1234567891011public BufferedInputStream(InputStream in) { this(in, DEFAULT_BUFFER_SIZE);}public BufferedInputStream(InputStream in, int size) { super(in); if (size &lt;= 0) { throw new IllegalArgumentException(&quot;Buffer size &lt;= 0&quot;); } buf = new byte[size];} 适配器模式适配器（Adapter Pattern）模式主要用于接口互不兼容的类的协调工作。适配器模式中存在被适配的对象或者类称为适配者(Adaptee) ，作用于适配者的对象或者类称为适配器(Adapter) 。 InputStream 和 OutputStream 的子类是被适配者， InputStreamReader 和 OutputStreamWriter 是适配器，主要通过实现被适配者的接口实现。 1234// InputStreamReader 是适配器，FileInputStream 是被适配的类InputStreamReader isr = new InputStreamReader(new FileInputStream(fileName), &quot;UTF-8&quot;);// BufferedReader 增强 InputStreamReader 的功能（装饰器模式）BufferedReader bufferedReader = new BufferedReader(isr); FutureTask 类使用了适配器模式，Executors 的内部类 RunnableAdapter 实现属于适配器，用于将 Runnable 适配成 Callable。 123456789101112131415161718192021222324252627public FutureTask(Runnable runnable, V result) { // 调用 Executors 类的 callable 方法 this.callable = Executors.callable(runnable, result); this.state = NEW;}// 实际调用的是 Executors 的内部类 RunnableAdapter 的构造方法public static &lt;T&gt; Callable&lt;T&gt; callable(Runnable task, T result) { if (task == null) throw new NullPointerException(); return new RunnableAdapter&lt;T&gt;(task, result);}// 适配器static final class RunnableAdapter&lt;T&gt; implements Callable&lt;T&gt; { final Runnable task; final T result; RunnableAdapter(Runnable task, T result) { this.task = task; this.result = result; } @Override public T call() { task.run(); return result; }} 享元模式享元模式实现复用对象，节省内存，前提享元对象是不可变的对象。与单例的不同在于可以一次提取一个或多个对象（结构式模式）。 代理模式外观模式桥接模式行为型模式模版模式模板方法是定义一个算法骨架，具体实现延迟到子类实现。从而提高代码的复用性，实现了反向控制，符合开闭原则。 迭代器模式策略模式策略模式是定义一系列的算法，将每个算法封装，使它们可以相互替换。 责任链模式责任链模式是将链中每一个节点看作是一个对象，每个节点处理的请求均不同，且内部自动维护一个下一节点对象。当一个请求从链的首端发出时，会沿着链的路径依次传递给每一个节点对象，直至有对象处理这个请求为止 。易于扩展，解耦，但是链路过长可能影响效率。 观察者模式观察者模式是定义对象间的一种一对多依赖关系，使得每当一个对象状态发生改变时，其相关 依赖对象皆得到通知并被自动更新。又称为发布-订阅模式。","link":"/2024/12/01/%E8%AF%A6%E7%BB%86%E8%AE%BE%E8%AE%A1/"},{"title":"深入解析 GPT 的工作原理","text":"前言GPT（Generative Pretrained Transformer）是一种基于 Transformer 架构 的自然语言处理模型。通过对用户输入的文本进行语义分析，GPT 能够生成连贯、符合上下文的回答。为了更好地理解 GPT 的工作原理，我们将其分为四个主要步骤：输入处理、Transformer 内部计算、自注意力机制、输出生成。本文将逐步解析这些步骤，尤其是 Transformer 的详细结构和机制，并通过具体示例说明每一步的作用。 输入处理：从自然语言到机器可理解的形式在用户向 GPT 提出问题时，模型需要首先将这些自然语言转换为机器能够处理的数值表示。这个过程涉及两个关键步骤：令牌化（Tokenization） 和 嵌入表示（Embeddings）。 令牌化是将输入文本分解为更小的单元——令牌（tokens）。这些令牌可以是单词、子词或字符片段。GPT 将输入文本逐词或逐子词分解，以便模型能够更细致地处理文本结构。 比如对于句子“为什么天空是蓝色的？”，令牌化后，可能被分解为 [为什么，天空，是，蓝色。的，?]。 为了提高处理能力，GPT 使用 子词令牌化方法，对于较长的词汇，模型可能将其拆解为多个子词。 将令牌化后的单词转化为数值表示是自然语言处理（NLP）中至关重要的一步。每个令牌通过嵌入层被映射为一个高维向量，向量的每个维度捕捉了该词的某种语义信息。这使得 GPT 能够处理自然语言的语义关系。这个步骤通常称为词嵌入（word embedding）或向量化（vectorization）。 这个过程有几种常见的实现方法： 独热编码（One-hot Encoding）：为每个词汇分配一个唯一的数字位置。每个令牌会转化为一个向量，其中只有对应单词位置的元素为 1，其它元素为 0。其优点在于简单直观，缺点在于高维稀疏，无法捕获词义。 词袋模型（Bag of Words，BoW）：通过统计文本中各个词汇的出现次数来表示文本。每个文档或句子被表示为一个词汇表中单词频率的向量。词袋模型忽略了单词的顺序，无法捕捉语法或上下文信息。当词汇表很大时，生成的向量会非常稀疏。 词嵌入（Word Embedding）：将每个单词映射到一个低维度的连续向量空间中。这些向量不仅具有数值表示，而且能够捕捉单词之间的语义和关系。流行的词嵌入方式包括： Word2Vec：通过上下文窗口来训练单词的向量表示，能够捕捉到单词间的相似性。例如，“king” - “man” + “woman” ≈ “queen”。 GloVe（Global Vectors for Word Representation）：基于统计信息和词频共现关系训练词向量。 FastText：不仅考虑单词，还考虑单词内部的子词，能够处理词形变化和拼写错误。 通过嵌入表示，模型能够理解词语的语义，并为后续的计算奠定基础。 Transformer 内部计算：核心架构的处理嵌入后的数值表示会被传入 GPT 的 Transformer 架构中，这一部分是模型理解输入的核心。Transformer 通过多层计算对输入句子进行深度分析，并生成相应的输出。 Transformer 是一种深度学习模型架构，首次由 Vaswani 等人于 2017 年在论文 Attention is All You Need 中提出。 Transformer 是由多层堆叠的 编码器-解码器（Encoder-Decoder）结构 演化而来，但 GPT 仅使用其中的 解码器 部分。Transformer 的每一层都包含两个主要组成部分： 自注意力机制（Self-Attention）：用来捕捉输入中词与词之间的关联。 前馈神经网络（Feed-Forward Network, FFN）：用来进行进一步的非线性变换，从而丰富语义表示。 在每一层 Transformer 中，嵌入表示被送入 自注意力机制，通过权重计算词语之间的关系。然后，经过前馈神经网络进一步处理。每一层的输出会作为下一层的输入，逐步强化对文本的语义理解。 这种多层处理使得 GPT 能够从表面词汇关联到更深层的语义关系，增强了其语言生成能力。 自注意力机制：模型理解上下文的基础自注意力机制是 Transformer 架构的关键组件，它使得 GPT 在生成每个词时，能够 “关注” 到与当前词相关的其他词。通过这种机制，模型可以捕捉到句子中远距离词语之间的依赖关系，并理解复杂的句子结构。 自注意力机制的工作原理可以这样表示。假设输入序列为：$[X_1, X_2, X_3,\\dots, X_n]$，其中 $X$ 表示每个词的嵌入表示。自注意力机制的目标是计算每个单词（例如 $X_1$）如何与其他单词（例如 $X_2$, $X_3$ 等）进行交互，并调整其表示。 每个输入单词的向量表示会被映射成三个不同的向量： 查询（Query）：表示当前词的查询向量。 键（Key）：表示其他词的键向量。 值（Value）：表示其他词的值向量。 这些向量是通过将输入词向量与学习到的权重矩阵相乘得到的。 模型计算每个查询（Query）和所有键（Key）之间的相似度，通常使用点积计算。这个得分表示了当前词对其他词的“注意力”程度。$$score(Q_i,K_j)=Q_i\\cdot K_j^T$$得分越高，表示 $Q_i$ 更加关注 $K_j$。 为了避免点积结果过大，通常会将得分除以一个常数（通常是键向量的维度的平方根）进行缩放：$$scaled_score(Q_i,K_j)=\\frac{Q_i\\cdot K_j^T}{\\sqrt{d_k}}⋅$$其中，$d_k$ 是键向量的维度。 对每个查询向量的得分进行 Softmax 操作，以确保所有的注意力得分和为 1，从而转化为概率分布：$$Attention_output_i=\\sum_jAttention_weights(Q_i,K_j)\\cdot V_j$$这一步骤的结果就是调整后的表示，它包含了序列中所有其他词的信息。 为了让模型能够在不同的子空间（不同角度）中学习信息，Transformer 引入了 多头注意力。它将查询、键和值分别拆分成多个头，每个头独立计算注意力，然后将所有头的结果拼接起来，经过线性变换得到最终的输出。 输出生成：从预测到生成完整回答经过多层 Transformer 和自注意力机制的处理，GPT 对输入的语义有了充分的理解，接下来进入 输出生成 阶段。GPT 的输出生成过程是逐词生成，并结合上下文进行推测。 GPT 使用自回归生成，即逐词生成下一个词，生成每个词时都会参考前面的上下文。每一步都会根据当前词语及其上下文信息，预测下一个最有可能的词。 在每次生成一个词时，GPT 会为所有可能的下一个词计算出一个 概率分布。每个词都有一定的概率被选中，模型会根据这些概率决定哪个词最适合作为下一个输出。 温度参数（Temperature Parameter）是控制文本生成模型（如 GPT、LSTM 等）输出随机性的一个超参数。 给定一个词的 logits（未经归一化的预测值），softmax 函数的输出是每个单词的概率：$$P(w_i)=\\frac{e^{z_i}}{\\sum_je^{z_j}}$$其中 $z_i$ 为对应单词的 logit。温度参数 $T$ 的引入修改了 softmax 函数，使得每个 logit 被除以一个温度值：$$P(w_i)=\\frac{e^{z_i/T}}{\\sum_je^{z_j/T}}$$当温度值低于 1 时，模型的概率分布变得更加“尖锐”。这意味着生成高概率单词的机会会增加，模型更倾向于选择最可能的单词。当温度值大于 1 时，模型生成低概率词的机会增大，生成的文本会更加随机、多样化，温度为 1 时，生成的概率分布是标准的，没有任何调节。 OpenAI o1：利用 LLM 学习推理OpenAI o1，这是一种新的大型语言模型，经过强化学习训练可以执行复杂的推理。OpenAI o1 系列已经开始展现出“类思考”的能力。OpenAI 甚至明确指出，“o1 不是 gpt-4o 的继任者”","link":"/2024/12/01/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90-GPT-%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/"}],"tags":[{"name":"设计模式","slug":"设计模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"AI","slug":"AI","link":"/tags/AI/"}],"categories":[],"pages":[]}